{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ee40012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5255ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BallAcceleration', 'Time', 'DistanceWall', 'DistanceCeil', 'DistanceBall', 'PlayerSpeed', 'BallSpeed', 'up', 'accelerate', 'slow', 'goal', 'left', 'boost', 'camera', 'down', 'right', 'slide', 'jump']\n"
     ]
    }
   ],
   "source": [
    "classes = ['noise','ceiling_shot','power_shot','waving_dash','air_dribble','front_flick','musty_flick']\n",
    "\n",
    "arq = open('rocket_league_skillshots.data', 'r')\n",
    "\n",
    "labels = arq.readline().split()\n",
    "\n",
    "if not (os.path.isdir('data')):\n",
    "    os.makedirs('data')\n",
    "    for d in classes:\n",
    "        new_dir = 'data/'+d\n",
    "        os.makedirs(new_dir)\n",
    "\n",
    "    quant = [0,0,0,0,0,0,0]\n",
    "\n",
    "    for linha in arq:\n",
    "\n",
    "        if (len(linha) < 5):\n",
    "            try:\n",
    "                dataset.close()\n",
    "            except:\n",
    "                True\n",
    "\n",
    "            if linha[0] == '-':\n",
    "                classe = 0\n",
    "            else:\n",
    "                classe = int(linha[0])\n",
    "\n",
    "            if classe>3:\n",
    "                classe -= 1\n",
    "\n",
    "            quant[classe] += 1\n",
    "\n",
    "            rota = 'data/'+classes[classe]+'/dataset'+str(quant[classe])+'.csv'\n",
    "\n",
    "\n",
    "            dataset = open(rota,'w')\n",
    "\n",
    "        else:\n",
    "            dataset.write(linha[:-2]+'\\n')\n",
    "\n",
    "\n",
    "    dataset.close()\n",
    "    \n",
    "print(labels)\n",
    "arq.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f9f2a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_list = []\n",
    "x_test_list = []\n",
    "\n",
    "y_train_list = []\n",
    "y_test_list = []\n",
    "\n",
    "for _, subp, _ in os.walk('data'):\n",
    "    for a in subp:\n",
    "        local = 'data/' + a\n",
    "    \n",
    "        for _, _, arqu in os.walk(local):\n",
    "            i = 0\n",
    "            \n",
    "            for b in arqu:\n",
    "                \n",
    "                local = 'data/' + a + '/' + b\n",
    "\n",
    "                temp = pd.read_csv(local, skiprows=0, sep=' ', names=labels)\n",
    "                    \n",
    "                \n",
    "                if (i+1)/len(arqu) < 0.8:\n",
    "                    x_train_list.append(temp.to_numpy())\n",
    "                    y_train_list.append(a)\n",
    "                    \n",
    "                else:\n",
    "                    x_test_list.append(temp.to_numpy())\n",
    "                    y_test_list.append(a)\n",
    "                \n",
    "                i+=1\n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "779751f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_array = np.array(y_train_list).reshape(-1,1)\n",
    "y_test_array = np.array(y_test_list).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcdcbf71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(233,) (233,)\n",
      "(65,) (65,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabriel.felix\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1970: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  result = asarray(a).shape\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(x_train_list), np.shape(y_train_list))\n",
    "print(np.shape(x_test_list), np.shape(y_test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbcded9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff4c6d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "for serie in x_train_list:\n",
    "    scaler.partial_fit(serie)\n",
    "    \n",
    "for serie_idx in range(len(x_train_list)):\n",
    "    x_train_list[serie_idx] = scaler.transform(x_train_list[serie_idx])\n",
    "    \n",
    "for serie_idx in range(len(x_test_list)):\n",
    "    x_test_list[serie_idx] = scaler.transform(x_test_list[serie_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e43b50",
   "metadata": {},
   "source": [
    "x_train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1ad21d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for serie_idx in range(len(x_train_list)):\n",
    "    x_train_list[serie_idx] = torch.tensor(np.array(x_train_list[serie_idx]), dtype=torch.float32)\n",
    "    \n",
    "for serie_idx in range(len(x_test_list)):\n",
    "    x_test_list[serie_idx] = torch.tensor(np.array(x_test_list[serie_idx]), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "706b0cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['air_dribble', 'ceiling_shot', 'front_flick', 'musty_flick',\n",
       "        'noise', 'power_shot', 'waving_dash'], dtype='<U12')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = OneHotEncoder()\n",
    "encoder.fit(y_train_array)\n",
    "encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2110cc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_array = encoder.transform(y_train_array).toarray()\n",
    "y_test_array = encoder.transform(y_test_array).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05b36321",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_x_train = pad_sequence(x_train_list)\n",
    "tensor_x_test = pad_sequence(x_test_list)\n",
    "\n",
    "tensor_y_train = torch.tensor(y_train_array, dtype=torch.float32)\n",
    "tensor_y_test = torch.tensor(y_test_array, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8c85f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 233, 18]) torch.Size([233, 7])\n",
      "torch.Size([57, 65, 18]) torch.Size([65, 7])\n"
     ]
    }
   ],
   "source": [
    "print(tensor_x_train.shape, tensor_y_train.shape)\n",
    "print(tensor_x_test.shape, tensor_y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6a01799",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tensor_x_train = tensor_x_train.reshape((tensor_x_train.shape[1],tensor_x_train.shape[0],-1))\n",
    "tensor_x_test = tensor_x_test.reshape((tensor_x_test.shape[1],tensor_x_test.shape[0],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1244ff7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([233, 64, 18]) torch.Size([233, 7])\n",
      "torch.Size([65, 57, 18]) torch.Size([65, 7])\n"
     ]
    }
   ],
   "source": [
    "print(tensor_x_train.shape, tensor_y_train.shape)\n",
    "print(tensor_x_test.shape, tensor_y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cc74155",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=50\n",
    "lr = 0.001\n",
    "n_epochs = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aec9561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(tensor_x_train, tensor_y_train)\n",
    "test_dataset = TensorDataset(tensor_x_test, tensor_y_test)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8e32ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN,self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=18, hidden_size=64, num_layers=4, batch_first=True, dropout=0.3)\n",
    "        \n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "        \n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "        \n",
    "        self.layer7 = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(16, 7),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "       \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        _,(out,_) = self.lstm(x)\n",
    "        out = self.layer5(out[3])\n",
    "        out = self.layer6(out)\n",
    "        out = self.layer7(out)\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dda0372c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NN(\n",
       "  (lstm): LSTM(18, 64, num_layers=4, batch_first=True, dropout=0.3)\n",
       "  (layer2): Sequential(\n",
       "    (0): Dropout(p=0.3, inplace=False)\n",
       "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Tanh()\n",
       "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Dropout(p=0.3, inplace=False)\n",
       "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Tanh()\n",
       "    (3): Linear(in_features=32, out_features=16, bias=True)\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Dropout(p=0.3, inplace=False)\n",
       "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Tanh()\n",
       "    (3): Linear(in_features=16, out_features=7, bias=True)\n",
       "    (4): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NN()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1fb26a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51d117b2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Acuracy: 0.1343030333518982, Loss: 1.9936789274215698\n",
      "Epoch: 2, Acuracy: 0.1403636336326599, Loss: 1.9706494808197021\n",
      "Epoch: 3, Acuracy: 0.17830303311347961, Loss: 1.940988540649414\n",
      "Epoch: 4, Acuracy: 0.20848484337329865, Loss: 1.9349268674850464\n",
      "Epoch: 5, Acuracy: 0.1903030276298523, Loss: 1.919585943222046\n",
      "Epoch: 6, Acuracy: 0.3065454661846161, Loss: 1.857787847518921\n",
      "Epoch: 7, Acuracy: 0.28460606932640076, Loss: 1.8292789459228516\n",
      "Epoch: 8, Acuracy: 0.33684849739074707, Loss: 1.7812261581420898\n",
      "Epoch: 9, Acuracy: 0.32472729682922363, Loss: 1.7802577018737793\n",
      "Epoch: 10, Acuracy: 0.3346666693687439, Loss: 1.7451391220092773\n",
      "Epoch: 11, Acuracy: 0.3346666395664215, Loss: 1.7356539964675903\n",
      "Epoch: 12, Acuracy: 0.3184242248535156, Loss: 1.733324646949768\n",
      "Epoch: 13, Acuracy: 0.3646060824394226, Loss: 1.696816325187683\n",
      "Epoch: 14, Acuracy: 0.3888484835624695, Loss: 1.6757203340530396\n",
      "Epoch: 15, Acuracy: 0.3146666884422302, Loss: 1.6692222356796265\n",
      "Epoch: 16, Acuracy: 0.3086060583591461, Loss: 1.7328898906707764\n",
      "Epoch: 17, Acuracy: 0.32872724533081055, Loss: 1.6780059337615967\n",
      "Epoch: 18, Acuracy: 0.36084848642349243, Loss: 1.67293119430542\n",
      "Epoch: 19, Acuracy: 0.34860605001449585, Loss: 1.6885137557983398\n",
      "Epoch: 20, Acuracy: 0.3364848494529724, Loss: 1.6815071105957031\n",
      "Epoch: 21, Acuracy: 0.34060606360435486, Loss: 1.6789283752441406\n",
      "Epoch: 22, Acuracy: 0.3288484811782837, Loss: 1.6698590517044067\n",
      "Epoch: 23, Acuracy: 0.3364848494529724, Loss: 1.6569414138793945\n",
      "Epoch: 24, Acuracy: 0.36084848642349243, Loss: 1.611606478691101\n",
      "Epoch: 25, Acuracy: 0.36242422461509705, Loss: 1.6183226108551025\n",
      "Epoch: 26, Acuracy: 0.3768484890460968, Loss: 1.5808649063110352\n",
      "Epoch: 27, Acuracy: 0.348727285861969, Loss: 1.593904972076416\n",
      "Epoch: 28, Acuracy: 0.3707878589630127, Loss: 1.5719667673110962\n",
      "Epoch: 29, Acuracy: 0.366787850856781, Loss: 1.568702220916748\n",
      "Epoch: 30, Acuracy: 0.40060606598854065, Loss: 1.5384341478347778\n",
      "Epoch: 31, Acuracy: 0.40278786420822144, Loss: 1.5535218715667725\n",
      "Epoch: 32, Acuracy: 0.3747878670692444, Loss: 1.5442026853561401\n",
      "Epoch: 33, Acuracy: 0.36872726678848267, Loss: 1.5325851440429688\n",
      "Epoch: 34, Acuracy: 0.4013333320617676, Loss: 1.5013447999954224\n",
      "Epoch: 35, Acuracy: 0.3726060688495636, Loss: 1.5110962390899658\n",
      "Epoch: 36, Acuracy: 0.4007272720336914, Loss: 1.533033847808838\n",
      "Epoch: 37, Acuracy: 0.39260604977607727, Loss: 1.5181289911270142\n",
      "Epoch: 38, Acuracy: 0.41866666078567505, Loss: 1.4786429405212402\n",
      "Epoch: 39, Acuracy: 0.3967272639274597, Loss: 1.4849858283996582\n",
      "Epoch: 40, Acuracy: 0.40872722864151, Loss: 1.483914852142334\n",
      "Epoch: 41, Acuracy: 0.41284850239753723, Loss: 1.4672397375106812\n",
      "Epoch: 42, Acuracy: 0.3928484618663788, Loss: 1.4962313175201416\n",
      "Epoch: 43, Acuracy: 0.412969708442688, Loss: 1.4687135219573975\n",
      "Epoch: 44, Acuracy: 0.3983030319213867, Loss: 1.4833176136016846\n",
      "Epoch: 45, Acuracy: 0.4368484914302826, Loss: 1.441455364227295\n",
      "Epoch: 46, Acuracy: 0.44303029775619507, Loss: 1.465318202972412\n",
      "Epoch: 47, Acuracy: 0.4471515417098999, Loss: 1.431135892868042\n",
      "Epoch: 48, Acuracy: 0.45866668224334717, Loss: 1.4270851612091064\n",
      "Epoch: 49, Acuracy: 0.43296968936920166, Loss: 1.432596206665039\n",
      "Epoch: 50, Acuracy: 0.44896969199180603, Loss: 1.4087541103363037\n",
      "Epoch: 51, Acuracy: 0.43090909719467163, Loss: 1.4395180940628052\n",
      "Epoch: 52, Acuracy: 0.46533334255218506, Loss: 1.4353597164154053\n",
      "Epoch: 53, Acuracy: 0.4710302948951721, Loss: 1.4066798686981201\n",
      "Epoch: 54, Acuracy: 0.43515151739120483, Loss: 1.4017927646636963\n",
      "Epoch: 55, Acuracy: 0.43515151739120483, Loss: 1.4190845489501953\n",
      "Epoch: 56, Acuracy: 0.462909072637558, Loss: 1.3871331214904785\n",
      "Epoch: 57, Acuracy: 0.47090908885002136, Loss: 1.3803470134735107\n",
      "Epoch: 58, Acuracy: 0.4768485128879547, Loss: 1.3571594953536987\n",
      "Epoch: 59, Acuracy: 0.44278788566589355, Loss: 1.3676148653030396\n",
      "Epoch: 60, Acuracy: 0.4790302813053131, Loss: 1.357459545135498\n",
      "Epoch: 61, Acuracy: 0.4867878556251526, Loss: 1.350815773010254\n",
      "Epoch: 62, Acuracy: 0.49115151166915894, Loss: 1.3685728311538696\n",
      "Epoch: 63, Acuracy: 0.46484848856925964, Loss: 1.3438843488693237\n",
      "Epoch: 64, Acuracy: 0.49503031373023987, Loss: 1.3140053749084473\n",
      "Epoch: 65, Acuracy: 0.4589090943336487, Loss: 1.3317842483520508\n",
      "Epoch: 66, Acuracy: 0.5153939127922058, Loss: 1.3301266431808472\n",
      "Epoch: 67, Acuracy: 0.47284847497940063, Loss: 1.347558617591858\n",
      "Epoch: 68, Acuracy: 0.5067878365516663, Loss: 1.3122377395629883\n",
      "Epoch: 69, Acuracy: 0.4729697108268738, Loss: 1.3250279426574707\n",
      "Epoch: 70, Acuracy: 0.4890909194946289, Loss: 1.340627670288086\n",
      "Epoch: 71, Acuracy: 0.4707878828048706, Loss: 1.3335038423538208\n",
      "Epoch: 72, Acuracy: 0.4810909032821655, Loss: 1.319419026374817\n",
      "Epoch: 73, Acuracy: 0.5329697132110596, Loss: 1.284754753112793\n",
      "Epoch: 74, Acuracy: 0.5149091482162476, Loss: 1.2932425737380981\n",
      "Epoch: 75, Acuracy: 0.5292121171951294, Loss: 1.2657983303070068\n",
      "Epoch: 76, Acuracy: 0.4790302813053131, Loss: 1.323448896408081\n",
      "Epoch: 77, Acuracy: 0.49539393186569214, Loss: 1.3052431344985962\n",
      "Epoch: 78, Acuracy: 0.49696967005729675, Loss: 1.2776798009872437\n",
      "Epoch: 79, Acuracy: 0.5009697079658508, Loss: 1.2267827987670898\n",
      "Epoch: 80, Acuracy: 0.5009697079658508, Loss: 1.2380526065826416\n",
      "Epoch: 81, Acuracy: 0.5431514978408813, Loss: 1.22127366065979\n",
      "Epoch: 82, Acuracy: 0.5151515007019043, Loss: 1.209473967552185\n",
      "Epoch: 83, Acuracy: 0.5592727661132812, Loss: 1.1693675518035889\n",
      "Epoch: 84, Acuracy: 0.5350303053855896, Loss: 1.1874698400497437\n",
      "Epoch: 85, Acuracy: 0.5715151429176331, Loss: 1.1767661571502686\n",
      "Epoch: 86, Acuracy: 0.5490909218788147, Loss: 1.1864595413208008\n",
      "Epoch: 87, Acuracy: 0.5209697484970093, Loss: 1.180612564086914\n",
      "Epoch: 88, Acuracy: 0.5693333148956299, Loss: 1.1402103900909424\n",
      "Epoch: 89, Acuracy: 0.5489696860313416, Loss: 1.1383570432662964\n",
      "Epoch: 90, Acuracy: 0.5350303053855896, Loss: 1.1884610652923584\n",
      "Epoch: 91, Acuracy: 0.5630303025245667, Loss: 1.143042802810669\n",
      "Epoch: 92, Acuracy: 0.545090913772583, Loss: 1.1568477153778076\n",
      "Epoch: 93, Acuracy: 0.5610909461975098, Loss: 1.1295182704925537\n",
      "Epoch: 94, Acuracy: 0.5671515464782715, Loss: 1.1523460149765015\n",
      "Epoch: 95, Acuracy: 0.5390303134918213, Loss: 1.121769666671753\n",
      "Epoch: 96, Acuracy: 0.555030345916748, Loss: 1.1210075616836548\n",
      "Epoch: 97, Acuracy: 0.5512727499008179, Loss: 1.1221271753311157\n",
      "Epoch: 98, Acuracy: 0.5572121143341064, Loss: 1.1253225803375244\n",
      "Epoch: 99, Acuracy: 0.5773333311080933, Loss: 1.1137325763702393\n",
      "Epoch: 100, Acuracy: 0.5370909571647644, Loss: 1.1457864046096802\n",
      "Epoch: 101, Acuracy: 0.5092121362686157, Loss: 1.2240878343582153\n",
      "Epoch: 102, Acuracy: 0.5389090776443481, Loss: 1.1330589056015015\n",
      "Epoch: 103, Acuracy: 0.5511515140533447, Loss: 1.139707088470459\n",
      "Epoch: 104, Acuracy: 0.5289697051048279, Loss: 1.1610387563705444\n",
      "Epoch: 105, Acuracy: 0.5250909328460693, Loss: 1.111825942993164\n",
      "Epoch: 106, Acuracy: 0.5369697213172913, Loss: 1.092146873474121\n",
      "Epoch: 107, Acuracy: 0.503151535987854, Loss: 1.192001461982727\n",
      "Epoch: 108, Acuracy: 0.5292121171951294, Loss: 1.152380108833313\n",
      "Epoch: 109, Acuracy: 0.5370908975601196, Loss: 1.096975326538086\n",
      "Epoch: 110, Acuracy: 0.5593939423561096, Loss: 1.1343508958816528\n",
      "Epoch: 111, Acuracy: 0.5369697213172913, Loss: 1.1261080503463745\n",
      "Epoch: 112, Acuracy: 0.5592727661132812, Loss: 1.0937047004699707\n",
      "Epoch: 113, Acuracy: 0.5671515464782715, Loss: 1.0578083992004395\n",
      "Epoch: 114, Acuracy: 0.545090913772583, Loss: 1.0632951259613037\n",
      "Epoch: 115, Acuracy: 0.5629091262817383, Loss: 1.09711492061615\n",
      "Epoch: 116, Acuracy: 0.5249696969985962, Loss: 1.0685161352157593\n",
      "Epoch: 117, Acuracy: 0.5452121496200562, Loss: 1.101343035697937\n",
      "Epoch: 118, Acuracy: 0.5973333716392517, Loss: 1.038830280303955\n",
      "Epoch: 119, Acuracy: 0.5730909109115601, Loss: 1.0287892818450928\n",
      "Epoch: 120, Acuracy: 0.5791515111923218, Loss: 1.0465489625930786\n",
      "Epoch: 121, Acuracy: 0.5492120981216431, Loss: 1.0972988605499268\n",
      "Epoch: 122, Acuracy: 0.5532121062278748, Loss: 1.038516879081726\n",
      "Epoch: 123, Acuracy: 0.6033939719200134, Loss: 1.0318903923034668\n",
      "Epoch: 124, Acuracy: 0.5592727065086365, Loss: 1.0422974824905396\n",
      "Epoch: 125, Acuracy: 0.5890909433364868, Loss: 1.0269379615783691\n",
      "Epoch: 126, Acuracy: 0.5892121195793152, Loss: 1.0148465633392334\n",
      "Epoch: 127, Acuracy: 0.5852121114730835, Loss: 1.0466091632843018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 128, Acuracy: 0.5693333148956299, Loss: 1.061985969543457\n",
      "Epoch: 129, Acuracy: 0.5572121143341064, Loss: 1.0065746307373047\n",
      "Epoch: 130, Acuracy: 0.5929697155952454, Loss: 1.0244868993759155\n",
      "Epoch: 131, Acuracy: 0.5770909190177917, Loss: 1.0133793354034424\n",
      "Epoch: 132, Acuracy: 0.547151505947113, Loss: 1.060319185256958\n",
      "Epoch: 133, Acuracy: 0.58921217918396, Loss: 1.0220102071762085\n",
      "Epoch: 134, Acuracy: 0.6392726898193359, Loss: 0.967419445514679\n",
      "Epoch: 135, Acuracy: 0.5973333120346069, Loss: 0.9734920263290405\n",
      "Epoch: 136, Acuracy: 0.5789090991020203, Loss: 1.0118108987808228\n",
      "Epoch: 137, Acuracy: 0.6012121438980103, Loss: 1.002647042274475\n",
      "Epoch: 138, Acuracy: 0.57103031873703, Loss: 1.0417556762695312\n",
      "Epoch: 139, Acuracy: 0.5932121276855469, Loss: 0.9994009733200073\n",
      "Epoch: 140, Acuracy: 0.6271514892578125, Loss: 0.9534534215927124\n",
      "Epoch: 141, Acuracy: 0.5709090828895569, Loss: 0.9812759757041931\n",
      "Epoch: 142, Acuracy: 0.5791515111923218, Loss: 1.000471591949463\n",
      "Epoch: 143, Acuracy: 0.5972121357917786, Loss: 0.9857999086380005\n",
      "Epoch: 144, Acuracy: 0.6152727603912354, Loss: 0.9532132148742676\n",
      "Epoch: 145, Acuracy: 0.6151515245437622, Loss: 0.9657709002494812\n",
      "Epoch: 146, Acuracy: 0.5970908999443054, Loss: 0.9432554841041565\n",
      "Epoch: 147, Acuracy: 0.6213333606719971, Loss: 0.957476019859314\n",
      "Epoch: 148, Acuracy: 0.6031515002250671, Loss: 0.9454590082168579\n",
      "Epoch: 149, Acuracy: 0.5630303025245667, Loss: 0.9837859869003296\n",
      "Epoch: 150, Acuracy: 0.6173333525657654, Loss: 0.964260458946228\n",
      "Epoch: 151, Acuracy: 0.6132121086120605, Loss: 0.9486997723579407\n",
      "Epoch: 152, Acuracy: 0.6311514973640442, Loss: 0.9089629650115967\n",
      "Epoch: 153, Acuracy: 0.635272741317749, Loss: 0.8971893787384033\n",
      "Epoch: 154, Acuracy: 0.6655757427215576, Loss: 0.8731943368911743\n",
      "Epoch: 155, Acuracy: 0.6031515002250671, Loss: 0.9143144488334656\n",
      "Epoch: 156, Acuracy: 0.5847272276878357, Loss: 0.9506142139434814\n",
      "Epoch: 157, Acuracy: 0.5972121357917786, Loss: 0.9786914587020874\n",
      "Epoch: 158, Acuracy: 0.5690909028053284, Loss: 0.9841632843017578\n",
      "Epoch: 159, Acuracy: 0.6413333415985107, Loss: 0.9182746410369873\n",
      "Epoch: 160, Acuracy: 0.623272716999054, Loss: 0.9281900525093079\n",
      "Epoch: 161, Acuracy: 0.6191515326499939, Loss: 0.9176112413406372\n",
      "Epoch: 162, Acuracy: 0.5810909271240234, Loss: 0.9308158755302429\n",
      "Epoch: 163, Acuracy: 0.6053333282470703, Loss: 0.9131492376327515\n",
      "Epoch: 164, Acuracy: 0.611393928527832, Loss: 0.9189764261245728\n",
      "Epoch: 165, Acuracy: 0.6333333253860474, Loss: 0.8951559066772461\n",
      "Epoch: 166, Acuracy: 0.6130909323692322, Loss: 0.9214829802513123\n",
      "Epoch: 167, Acuracy: 0.6191515326499939, Loss: 0.937067985534668\n",
      "Epoch: 168, Acuracy: 0.6214545965194702, Loss: 0.8935153484344482\n",
      "Epoch: 169, Acuracy: 0.6090909242630005, Loss: 0.884890615940094\n",
      "Epoch: 170, Acuracy: 0.6390303373336792, Loss: 0.8566121459007263\n",
      "Epoch: 171, Acuracy: 0.6250909566879272, Loss: 0.877228856086731\n",
      "Epoch: 172, Acuracy: 0.661212146282196, Loss: 0.8946354389190674\n",
      "Epoch: 173, Acuracy: 0.6452121734619141, Loss: 0.8763771653175354\n",
      "Epoch: 174, Acuracy: 0.6393939256668091, Loss: 0.9047191143035889\n",
      "Epoch: 175, Acuracy: 0.6416969895362854, Loss: 0.8911941051483154\n",
      "Epoch: 176, Acuracy: 0.6290909051895142, Loss: 0.899918794631958\n",
      "Epoch: 177, Acuracy: 0.6092121005058289, Loss: 0.8938308358192444\n",
      "Epoch: 178, Acuracy: 0.6755151748657227, Loss: 0.8741886615753174\n",
      "Epoch: 179, Acuracy: 0.6309090852737427, Loss: 0.8513186573982239\n",
      "Epoch: 180, Acuracy: 0.5949090719223022, Loss: 0.9042760133743286\n",
      "Epoch: 181, Acuracy: 0.6215757131576538, Loss: 0.887397050857544\n",
      "Epoch: 182, Acuracy: 0.6310303211212158, Loss: 0.8798967599868774\n",
      "Epoch: 183, Acuracy: 0.6033939123153687, Loss: 0.8846443891525269\n",
      "Epoch: 184, Acuracy: 0.6452121138572693, Loss: 0.8631731271743774\n",
      "Epoch: 185, Acuracy: 0.645575761795044, Loss: 0.8356300592422485\n",
      "Epoch: 186, Acuracy: 0.6715151071548462, Loss: 0.8401384353637695\n",
      "Epoch: 187, Acuracy: 0.6293333172798157, Loss: 0.8238954544067383\n",
      "Epoch: 188, Acuracy: 0.6593939065933228, Loss: 0.8378731608390808\n",
      "Epoch: 189, Acuracy: 0.6632727384567261, Loss: 0.7931692600250244\n",
      "Epoch: 190, Acuracy: 0.6872727274894714, Loss: 0.8199041485786438\n",
      "Epoch: 191, Acuracy: 0.6717575788497925, Loss: 0.8156037330627441\n",
      "Epoch: 192, Acuracy: 0.6734545230865479, Loss: 0.8104700446128845\n",
      "Epoch: 193, Acuracy: 0.635272741317749, Loss: 0.8935351371765137\n",
      "Epoch: 194, Acuracy: 0.6372121572494507, Loss: 0.8595432043075562\n",
      "Epoch: 195, Acuracy: 0.6934546232223511, Loss: 0.8028057813644409\n",
      "Epoch: 196, Acuracy: 0.6615757942199707, Loss: 0.8691718578338623\n",
      "Epoch: 197, Acuracy: 0.6170909404754639, Loss: 0.900964617729187\n",
      "Epoch: 198, Acuracy: 0.5947878956794739, Loss: 0.9865034818649292\n",
      "Epoch: 199, Acuracy: 0.3964848518371582, Loss: 1.5910581350326538\n",
      "Epoch: 200, Acuracy: 0.5072727203369141, Loss: 1.3054695129394531\n",
      "Epoch: 201, Acuracy: 0.5592727661132812, Loss: 1.1720906496047974\n",
      "Epoch: 202, Acuracy: 0.5271515250205994, Loss: 1.1305619478225708\n",
      "Epoch: 203, Acuracy: 0.5147879123687744, Loss: 1.0989773273468018\n",
      "Epoch: 204, Acuracy: 0.5353939533233643, Loss: 1.03343665599823\n",
      "Epoch: 205, Acuracy: 0.5410909056663513, Loss: 1.049039363861084\n",
      "Epoch: 206, Acuracy: 0.5048484802246094, Loss: 1.0906589031219482\n",
      "Epoch: 207, Acuracy: 0.5413333177566528, Loss: 1.0729020833969116\n",
      "Epoch: 208, Acuracy: 0.561090886592865, Loss: 1.061314582824707\n",
      "Epoch: 209, Acuracy: 0.5650908946990967, Loss: 0.9979522228240967\n",
      "Epoch: 210, Acuracy: 0.5792727470397949, Loss: 1.0227702856063843\n",
      "Epoch: 211, Acuracy: 0.5649696588516235, Loss: 1.066579818725586\n",
      "Epoch: 212, Acuracy: 0.5809696912765503, Loss: 1.0094029903411865\n",
      "Epoch: 213, Acuracy: 0.5652121305465698, Loss: 1.0408644676208496\n",
      "Epoch: 214, Acuracy: 0.5551515221595764, Loss: 1.0400927066802979\n",
      "Epoch: 215, Acuracy: 0.5932121276855469, Loss: 0.9779025316238403\n",
      "Epoch: 216, Acuracy: 0.5932121276855469, Loss: 1.0085978507995605\n",
      "Epoch: 217, Acuracy: 0.5672727227210999, Loss: 1.012979507446289\n",
      "Epoch: 218, Acuracy: 0.5573333501815796, Loss: 1.0099740028381348\n",
      "Epoch: 219, Acuracy: 0.5510303378105164, Loss: 1.0168838500976562\n",
      "Epoch: 220, Acuracy: 0.5312727093696594, Loss: 1.117934226989746\n",
      "Epoch: 221, Acuracy: 0.5431514978408813, Loss: 1.1609491109848022\n",
      "Epoch: 222, Acuracy: 0.540969729423523, Loss: 1.0911262035369873\n",
      "Epoch: 223, Acuracy: 0.547151505947113, Loss: 1.1049919128417969\n",
      "Epoch: 224, Acuracy: 0.5389090776443481, Loss: 1.066778540611267\n",
      "Epoch: 225, Acuracy: 0.5129697322845459, Loss: 1.018394947052002\n",
      "Epoch: 226, Acuracy: 0.5732121467590332, Loss: 1.0125163793563843\n",
      "Epoch: 227, Acuracy: 0.5889697074890137, Loss: 0.9927771091461182\n",
      "Epoch: 228, Acuracy: 0.6013333201408386, Loss: 0.9581299424171448\n",
      "Epoch: 229, Acuracy: 0.59333336353302, Loss: 0.9573817849159241\n",
      "Epoch: 230, Acuracy: 0.5569697022438049, Loss: 0.9643981456756592\n",
      "Epoch: 231, Acuracy: 0.5871515274047852, Loss: 1.0311442613601685\n",
      "Epoch: 232, Acuracy: 0.5772120952606201, Loss: 0.9954453110694885\n",
      "Epoch: 233, Acuracy: 0.5529696941375732, Loss: 1.008141040802002\n",
      "Epoch: 234, Acuracy: 0.6030303239822388, Loss: 0.9574098587036133\n",
      "Epoch: 235, Acuracy: 0.6174545288085938, Loss: 0.9457823038101196\n",
      "Epoch: 236, Acuracy: 0.5569697022438049, Loss: 0.9858142733573914\n",
      "Epoch: 237, Acuracy: 0.6052120923995972, Loss: 0.955878496170044\n",
      "Epoch: 238, Acuracy: 0.6132121086120605, Loss: 0.8990731835365295\n",
      "Epoch: 239, Acuracy: 0.5949090719223022, Loss: 0.9518594741821289\n",
      "Epoch: 240, Acuracy: 0.5892121195793152, Loss: 0.9473408460617065\n",
      "Epoch: 241, Acuracy: 0.5649697184562683, Loss: 0.9861326217651367\n",
      "Epoch: 242, Acuracy: 0.545090913772583, Loss: 0.9699728488922119\n",
      "Epoch: 243, Acuracy: 0.5789090991020203, Loss: 0.9410296082496643\n",
      "Epoch: 244, Acuracy: 0.6090909242630005, Loss: 0.9207701086997986\n",
      "Epoch: 245, Acuracy: 0.6172121167182922, Loss: 0.8678652048110962\n",
      "Epoch: 246, Acuracy: 0.6151515245437622, Loss: 0.8822273015975952\n",
      "Epoch: 247, Acuracy: 0.625333309173584, Loss: 0.9140850305557251\n",
      "Epoch: 248, Acuracy: 0.6353939175605774, Loss: 0.8660380840301514\n",
      "Epoch: 249, Acuracy: 0.6515151262283325, Loss: 0.862189531326294\n",
      "Epoch: 250, Acuracy: 0.6233939528465271, Loss: 0.92917400598526\n",
      "Epoch: 251, Acuracy: 0.5729697346687317, Loss: 1.036085844039917\n",
      "Epoch: 252, Acuracy: 0.5530909299850464, Loss: 1.0330196619033813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 253, Acuracy: 0.5671514868736267, Loss: 1.0819978713989258\n",
      "Epoch: 254, Acuracy: 0.5769697427749634, Loss: 0.9651488065719604\n",
      "Epoch: 255, Acuracy: 0.6092121005058289, Loss: 0.9706096649169922\n",
      "Epoch: 256, Acuracy: 0.5991514921188354, Loss: 0.8955572843551636\n",
      "Epoch: 257, Acuracy: 0.6212121248245239, Loss: 0.8946129083633423\n",
      "Epoch: 258, Acuracy: 0.6010909080505371, Loss: 0.9322410225868225\n",
      "Epoch: 259, Acuracy: 0.6111515164375305, Loss: 0.849977970123291\n",
      "Epoch: 260, Acuracy: 0.6738181710243225, Loss: 0.8520421981811523\n",
      "Epoch: 261, Acuracy: 0.623272716999054, Loss: 0.8606806397438049\n",
      "Epoch: 262, Acuracy: 0.635272741317749, Loss: 0.8707625865936279\n",
      "Epoch: 263, Acuracy: 0.6453333497047424, Loss: 0.8054243326187134\n",
      "Epoch: 264, Acuracy: 0.6213333606719971, Loss: 0.8479413986206055\n",
      "Epoch: 265, Acuracy: 0.6493333578109741, Loss: 0.8591996431350708\n",
      "Epoch: 266, Acuracy: 0.661212146282196, Loss: 0.880102813243866\n",
      "Epoch: 267, Acuracy: 0.6032727360725403, Loss: 0.8898178339004517\n",
      "Epoch: 268, Acuracy: 0.6052120923995972, Loss: 0.9027641415596008\n",
      "Epoch: 269, Acuracy: 0.6152727603912354, Loss: 0.8754202127456665\n",
      "Epoch: 270, Acuracy: 0.6252121329307556, Loss: 0.8427271842956543\n",
      "Epoch: 271, Acuracy: 0.6332120895385742, Loss: 0.8780947327613831\n",
      "Epoch: 272, Acuracy: 0.637333333492279, Loss: 0.8836747407913208\n",
      "Epoch: 273, Acuracy: 0.6230303049087524, Loss: 0.8992809057235718\n",
      "Epoch: 274, Acuracy: 0.6655757427215576, Loss: 0.8022990226745605\n",
      "Epoch: 275, Acuracy: 0.623272716999054, Loss: 0.8439972400665283\n",
      "Epoch: 276, Acuracy: 0.6952727437019348, Loss: 0.8009593486785889\n",
      "Epoch: 277, Acuracy: 0.6653333306312561, Loss: 0.8140014410018921\n",
      "Epoch: 278, Acuracy: 0.6253333687782288, Loss: 0.8417933583259583\n",
      "Epoch: 279, Acuracy: 0.6653333306312561, Loss: 0.8125054240226746\n",
      "Epoch: 280, Acuracy: 0.6453332901000977, Loss: 0.793882429599762\n",
      "Epoch: 281, Acuracy: 0.6572121381759644, Loss: 0.8136248588562012\n",
      "Epoch: 282, Acuracy: 0.6269091367721558, Loss: 0.8382267355918884\n",
      "Epoch: 283, Acuracy: 0.6193939447402954, Loss: 0.8484152555465698\n",
      "Epoch: 284, Acuracy: 0.6229090690612793, Loss: 0.8450674414634705\n",
      "Epoch: 285, Acuracy: 0.633212149143219, Loss: 0.8636178970336914\n",
      "Epoch: 286, Acuracy: 0.6773332953453064, Loss: 0.8172016143798828\n",
      "Epoch: 287, Acuracy: 0.6133333444595337, Loss: 0.8906921744346619\n",
      "Epoch: 288, Acuracy: 0.635272741317749, Loss: 0.8635790348052979\n",
      "Epoch: 289, Acuracy: 0.6374545097351074, Loss: 0.8850226402282715\n",
      "Epoch: 290, Acuracy: 0.6150302886962891, Loss: 0.8984228372573853\n",
      "Epoch: 291, Acuracy: 0.6089697480201721, Loss: 0.8609517812728882\n",
      "Epoch: 292, Acuracy: 0.6272727251052856, Loss: 0.8714786171913147\n",
      "Epoch: 293, Acuracy: 0.665454626083374, Loss: 0.7905142307281494\n",
      "Epoch: 294, Acuracy: 0.6153939366340637, Loss: 0.8688762187957764\n",
      "Epoch: 295, Acuracy: 0.5812121629714966, Loss: 0.9710574746131897\n",
      "Epoch: 296, Acuracy: 0.6416969895362854, Loss: 0.8249689936637878\n",
      "Epoch: 297, Acuracy: 0.655515193939209, Loss: 0.7737065553665161\n",
      "Epoch: 298, Acuracy: 0.6351515054702759, Loss: 0.8904547095298767\n",
      "Epoch: 299, Acuracy: 0.6254545450210571, Loss: 0.8493433594703674\n",
      "Epoch: 300, Acuracy: 0.48690909147262573, Loss: 1.4142937660217285\n",
      "Epoch: 301, Acuracy: 0.4729697108268738, Loss: 1.2999061346054077\n",
      "Epoch: 302, Acuracy: 0.46884846687316895, Loss: 1.2668691873550415\n",
      "Epoch: 303, Acuracy: 0.4990302622318268, Loss: 1.209486722946167\n",
      "Epoch: 304, Acuracy: 0.5130909085273743, Loss: 1.2093803882598877\n",
      "Epoch: 305, Acuracy: 0.48896971344947815, Loss: 1.2269595861434937\n",
      "Epoch: 306, Acuracy: 0.5129697322845459, Loss: 1.1818726062774658\n",
      "Epoch: 307, Acuracy: 0.3367272615432739, Loss: 1.7977122068405151\n",
      "Epoch: 308, Acuracy: 0.3909091055393219, Loss: 1.5410912036895752\n",
      "Epoch: 309, Acuracy: 0.3447272777557373, Loss: 1.7262799739837646\n",
      "Epoch: 310, Acuracy: 0.39078789949417114, Loss: 1.5262229442596436\n",
      "Epoch: 311, Acuracy: 0.39296969771385193, Loss: 1.5013446807861328\n",
      "Epoch: 312, Acuracy: 0.4946666955947876, Loss: 1.3434680700302124\n",
      "Epoch: 313, Acuracy: 0.4790303111076355, Loss: 1.266154170036316\n",
      "Epoch: 314, Acuracy: 0.41490909457206726, Loss: 1.2906644344329834\n",
      "Epoch: 315, Acuracy: 0.45284849405288696, Loss: 1.1905791759490967\n",
      "Epoch: 316, Acuracy: 0.5070303082466125, Loss: 1.157509207725525\n",
      "Epoch: 317, Acuracy: 0.4591515064239502, Loss: 1.2501447200775146\n",
      "Epoch: 318, Acuracy: 0.4729696810245514, Loss: 1.3090623617172241\n",
      "Epoch: 319, Acuracy: 0.5272727012634277, Loss: 1.1825627088546753\n",
      "Epoch: 320, Acuracy: 0.45478785037994385, Loss: 1.1538214683532715\n",
      "Epoch: 321, Acuracy: 0.5208485126495361, Loss: 1.1322600841522217\n",
      "Epoch: 322, Acuracy: 0.5489697456359863, Loss: 1.0471630096435547\n",
      "Epoch: 323, Acuracy: 0.5715152025222778, Loss: 1.041092038154602\n",
      "Epoch: 324, Acuracy: 0.6032726764678955, Loss: 0.9680678844451904\n",
      "Epoch: 325, Acuracy: 0.555030345916748, Loss: 0.9809741377830505\n",
      "Epoch: 326, Acuracy: 0.5872727036476135, Loss: 0.951646625995636\n",
      "Epoch: 327, Acuracy: 0.5692121386528015, Loss: 1.014499545097351\n",
      "Epoch: 328, Acuracy: 0.5391515493392944, Loss: 1.176342487335205\n",
      "Epoch: 329, Acuracy: 0.5049697160720825, Loss: 1.1544787883758545\n",
      "Epoch: 330, Acuracy: 0.561090886592865, Loss: 1.0800281763076782\n",
      "Epoch: 331, Acuracy: 0.6415757536888123, Loss: 0.8819300532341003\n",
      "Epoch: 332, Acuracy: 0.44496968388557434, Loss: 1.1996748447418213\n",
      "Epoch: 333, Acuracy: 0.4750303030014038, Loss: 1.3627121448516846\n",
      "Epoch: 334, Acuracy: 0.43878787755966187, Loss: 1.3377950191497803\n",
      "Epoch: 335, Acuracy: 0.4647272825241089, Loss: 1.2708635330200195\n",
      "Epoch: 336, Acuracy: 0.5093333125114441, Loss: 1.2085820436477661\n",
      "Epoch: 337, Acuracy: 0.5272727012634277, Loss: 1.1123842000961304\n",
      "Epoch: 338, Acuracy: 0.5575757622718811, Loss: 1.0358963012695312\n",
      "Epoch: 339, Acuracy: 0.5568484663963318, Loss: 1.11162269115448\n",
      "Epoch: 340, Acuracy: 0.48872727155685425, Loss: 1.1171326637268066\n",
      "Epoch: 341, Acuracy: 0.5169697403907776, Loss: 1.061323881149292\n",
      "Epoch: 342, Acuracy: 0.5387879014015198, Loss: 1.0837304592132568\n",
      "Epoch: 343, Acuracy: 0.4907878637313843, Loss: 1.1230660676956177\n",
      "Epoch: 344, Acuracy: 0.59333336353302, Loss: 0.9928074479103088\n",
      "Epoch: 345, Acuracy: 0.5911515355110168, Loss: 0.963222861289978\n",
      "Epoch: 346, Acuracy: 0.5952727198600769, Loss: 0.9208875298500061\n",
      "Epoch: 347, Acuracy: 0.5511515140533447, Loss: 1.0151394605636597\n",
      "Epoch: 348, Acuracy: 0.5912727117538452, Loss: 0.9339697957038879\n",
      "Epoch: 349, Acuracy: 0.5591514706611633, Loss: 0.9978958964347839\n",
      "Epoch: 350, Acuracy: 0.6031515002250671, Loss: 0.9114639163017273\n",
      "Epoch: 351, Acuracy: 0.6393939256668091, Loss: 0.8800871968269348\n",
      "Epoch: 352, Acuracy: 0.6089696884155273, Loss: 0.9447333812713623\n",
      "Epoch: 353, Acuracy: 0.6152727007865906, Loss: 0.907878577709198\n",
      "Epoch: 354, Acuracy: 0.5449697375297546, Loss: 1.0470843315124512\n",
      "Epoch: 355, Acuracy: 0.5670303106307983, Loss: 1.002332329750061\n",
      "Epoch: 356, Acuracy: 0.5670303106307983, Loss: 1.0256006717681885\n",
      "Epoch: 357, Acuracy: 0.5671515464782715, Loss: 0.9842976331710815\n",
      "Epoch: 358, Acuracy: 0.6012121438980103, Loss: 0.9345880746841431\n",
      "Epoch: 359, Acuracy: 0.5787878632545471, Loss: 0.9851988554000854\n",
      "Epoch: 360, Acuracy: 0.5389090776443481, Loss: 1.0115771293640137\n",
      "Epoch: 361, Acuracy: 0.5529696941375732, Loss: 1.012328028678894\n",
      "Epoch: 362, Acuracy: 0.5852121114730835, Loss: 0.9309815168380737\n",
      "Epoch: 363, Acuracy: 0.5792727470397949, Loss: 0.9667771458625793\n",
      "Epoch: 364, Acuracy: 0.5949090719223022, Loss: 0.9492675065994263\n",
      "Epoch: 365, Acuracy: 0.6513938903808594, Loss: 0.9277886152267456\n",
      "Epoch: 366, Acuracy: 0.637333333492279, Loss: 0.8483442664146423\n",
      "Epoch: 367, Acuracy: 0.6132121086120605, Loss: 0.8982837200164795\n",
      "Epoch: 368, Acuracy: 0.6534545421600342, Loss: 0.8727540969848633\n",
      "Epoch: 369, Acuracy: 0.6373332738876343, Loss: 0.8383629322052002\n",
      "Epoch: 370, Acuracy: 0.6632727384567261, Loss: 0.8361800909042358\n",
      "Epoch: 371, Acuracy: 0.5951514840126038, Loss: 0.878457248210907\n",
      "Epoch: 372, Acuracy: 0.6191515326499939, Loss: 0.8664876222610474\n",
      "Epoch: 373, Acuracy: 0.6332120895385742, Loss: 0.8713017702102661\n",
      "Epoch: 374, Acuracy: 0.6713939309120178, Loss: 0.8293279409408569\n",
      "Epoch: 375, Acuracy: 0.6773333549499512, Loss: 0.8192906975746155\n",
      "Epoch: 376, Acuracy: 0.6535757780075073, Loss: 0.8485713601112366\n",
      "Epoch: 377, Acuracy: 0.6350303292274475, Loss: 0.9119226932525635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 378, Acuracy: 0.6693333387374878, Loss: 0.8319841027259827\n",
      "Epoch: 379, Acuracy: 0.6673939228057861, Loss: 0.8086471557617188\n",
      "Epoch: 380, Acuracy: 0.5907878875732422, Loss: 0.921393871307373\n",
      "Epoch: 381, Acuracy: 0.5973333120346069, Loss: 0.872602641582489\n",
      "Epoch: 382, Acuracy: 0.6370909214019775, Loss: 0.8467140793800354\n",
      "Epoch: 383, Acuracy: 0.6293333768844604, Loss: 0.8332197666168213\n",
      "Epoch: 384, Acuracy: 0.6795151233673096, Loss: 0.7890780568122864\n",
      "Epoch: 385, Acuracy: 0.6494545936584473, Loss: 0.8034728765487671\n",
      "Epoch: 386, Acuracy: 0.6452121138572693, Loss: 0.84373539686203\n",
      "Epoch: 387, Acuracy: 0.6915152072906494, Loss: 0.7995949983596802\n",
      "Epoch: 388, Acuracy: 0.697454571723938, Loss: 0.7881227135658264\n",
      "Epoch: 389, Acuracy: 0.6694545745849609, Loss: 0.7918943166732788\n",
      "Epoch: 390, Acuracy: 0.6792727112770081, Loss: 0.7933754920959473\n",
      "Epoch: 391, Acuracy: 0.673575758934021, Loss: 0.8178136348724365\n",
      "Epoch: 392, Acuracy: 0.6733332872390747, Loss: 0.7753191590309143\n",
      "Epoch: 393, Acuracy: 0.6773333549499512, Loss: 0.788849413394928\n",
      "Epoch: 394, Acuracy: 0.6775757074356079, Loss: 0.8007180094718933\n",
      "Epoch: 395, Acuracy: 0.625333309173584, Loss: 0.8512814044952393\n",
      "Epoch: 396, Acuracy: 0.6532121300697327, Loss: 0.8026937246322632\n",
      "Epoch: 397, Acuracy: 0.6133333444595337, Loss: 0.9003384113311768\n",
      "Epoch: 398, Acuracy: 0.6313939094543457, Loss: 0.8421709537506104\n",
      "Epoch: 399, Acuracy: 0.6249697208404541, Loss: 0.8736594319343567\n",
      "Epoch: 400, Acuracy: 0.6132121086120605, Loss: 0.8886542916297913\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    loss_epoch=0\n",
    "    acc_epoch=0\n",
    "    \n",
    "    \n",
    "    for data, target in train_dataloader:\n",
    "        output = model(data)\n",
    "        loss = criterion(output,target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_epoch += loss/len(train_dataloader)\n",
    "        acc = (output.argmax(dim=1) == target.argmax(dim=1)).float().mean()\n",
    "        acc_epoch += acc/len(train_dataloader)\n",
    "        \n",
    "    writer.add_scalar('Accuracy', acc_epoch, epoch)\n",
    "    writer.add_scalar('Loss', loss_epoch, epoch)\n",
    "        \n",
    "    print('Epoch: {}, Acuracy: {}, Loss: {}'.format(epoch+1, acc_epoch, loss_epoch))\n",
    "    \n",
    "writer.flush()\n",
    "writer.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb64e69d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NN(\n",
       "  (lstm): LSTM(18, 64, num_layers=4, batch_first=True, dropout=0.3)\n",
       "  (layer2): Sequential(\n",
       "    (0): Dropout(p=0.3, inplace=False)\n",
       "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Tanh()\n",
       "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Dropout(p=0.3, inplace=False)\n",
       "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Tanh()\n",
       "    (3): Linear(in_features=32, out_features=16, bias=True)\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Dropout(p=0.3, inplace=False)\n",
       "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Tanh()\n",
       "    (3): Linear(in_features=16, out_features=7, bias=True)\n",
       "    (4): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9932dc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7333333492279053 \n",
      "Test Loss: 0.6290507316589355\n"
     ]
    }
   ],
   "source": [
    "loss_test=0\n",
    "acc_test=0\n",
    "\n",
    "for data, target in test_dataloader:\n",
    "    output = model(data)\n",
    "    loss = criterion(output,target)\n",
    "\n",
    "    loss_test += loss/len(test_dataloader)\n",
    "    acc = (output.argmax(dim=1) == target.argmax(dim=1)).float().mean()\n",
    "    acc_test += acc/len(test_dataloader)\n",
    "\n",
    "print('Test Accuracy: {} \\nTest Loss: {}'.format(acc_test, loss_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
